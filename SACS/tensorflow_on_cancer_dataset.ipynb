{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\"C:/Users/Nill/Desktop/datasets/data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "\n",
    "    #independet variable or matrix of features (index from 2 to 31)\n",
    "    X = dataset[dataset.columns[2:32]].values\n",
    "\n",
    "    #get the dependent variable that has an index of 1\n",
    "    y1 = dataset[dataset.columns[1]]\n",
    "\n",
    "    #encode the dependent variable\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y1)\n",
    "    \n",
    "    y = encoder.transform(y1)\n",
    "    Y = one_hot_encode(y)\n",
    "    \n",
    "    print(X.shape)\n",
    "    return (X, Y, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the encoder function\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "#read the dataset\n",
    "X, Y, y1 = read_dataset()\n",
    "#shuffle the dataset\n",
    "X, Y = shuffle(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#create StandaradScaler object\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "#fit it to the train dataset\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "#no need to fit to the test set (since its already fitted in train set)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(455, 2)\n",
      "(114, 30)\n"
     ]
    }
   ],
   "source": [
    "#inspect the shape of training and testing data\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_dim =   30\n"
     ]
    }
   ],
   "source": [
    "#defining the important parameters and variable to work with the tensors\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "\n",
    "#loss function\n",
    "cost_history = np.empty(shape=[1], dtype=float)\n",
    "\n",
    "#columns in X\n",
    "n_dim = X.shape[1]\n",
    "\n",
    "#will print all the column in X variable\n",
    "print(\"n_dim\", \"=\",\" \", n_dim)\n",
    "\n",
    "#malignant or benign\n",
    "n_class = 2\n",
    "\n",
    "model_path = 'C:/Users/Nill/Desktop/conda_jupyter/data.cvv_models/TFM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the number of hidden layers and neurons for each layer\n",
    "\n",
    "#multi-layer perceptrons\n",
    "n_hidden_1 = 60\n",
    "n_hidden_2 = 60\n",
    "n_hidden_3 = 60\n",
    "n_hidden_4 = 60\n",
    "\n",
    "#we will feed in the dataset\n",
    "x = tf.placeholder(tf.float32, [None, n_dim])\n",
    "\n",
    "#w will be initialized with zeros\n",
    "W = tf.Variable(tf.zeros([n_dim, n_class]))\n",
    "\n",
    "#b will be initialized with zeros with a shape of n_class\n",
    "b = tf.Variable(tf.zeros([n_class]))\n",
    "\n",
    "#y_out is the output of the model that we already know\n",
    "y_out = tf.placeholder(tf.float32, [None, n_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    \n",
    "    #Hidden layers\n",
    "    \n",
    "    #first layer will be the matrix multiplication of input and weight, then added to the bias\n",
    "    #initiated with sigmoid activation function\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    #for second hidden layer, the input will be the output of first hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    \n",
    "    #for third hidden layer, the input will be the output of second hidden layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    \n",
    "    #for fourth hidden layer, the input will be the output of third hidden layer\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    \n",
    "    #final output layer with linear activation function\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the weights and biases for each layer\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_class]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  -  cost:  8.23681  - MSE:  155.633901953  - Train Accuary:  0.628571\n",
      "epoch:  1  -  cost:  7.76911  - MSE:  142.796411873  - Train Accuary:  0.624176\n",
      "epoch:  2  -  cost:  7.30619  - MSE:  130.745771716  - Train Accuary:  0.624176\n",
      "epoch:  3  -  cost:  6.84967  - MSE:  119.494359661  - Train Accuary:  0.628571\n",
      "epoch:  4  -  cost:  6.40154  - MSE:  109.039603362  - Train Accuary:  0.630769\n",
      "epoch:  5  -  cost:  5.96417  - MSE:  99.3890841146  - Train Accuary:  0.628571\n",
      "epoch:  6  -  cost:  5.54093  - MSE:  90.5485756476  - Train Accuary:  0.621978\n",
      "epoch:  7  -  cost:  5.13674  - MSE:  82.5347070413  - Train Accuary:  0.628571\n",
      "epoch:  8  -  cost:  4.7568  - MSE:  75.3534531822  - Train Accuary:  0.637363\n",
      "epoch:  9  -  cost:  4.40437  - MSE:  68.9910895296  - Train Accuary:  0.63956\n",
      "epoch:  10  -  cost:  4.08242  - MSE:  63.4182681426  - Train Accuary:  0.650549\n",
      "epoch:  11  -  cost:  3.79182  - MSE:  58.5917388797  - Train Accuary:  0.652747\n",
      "epoch:  12  -  cost:  3.53059  - MSE:  54.4388293182  - Train Accuary:  0.650549\n",
      "epoch:  13  -  cost:  3.29672  - MSE:  50.8861337053  - Train Accuary:  0.665934\n",
      "epoch:  14  -  cost:  3.08911  - MSE:  47.8717369564  - Train Accuary:  0.672527\n",
      "epoch:  15  -  cost:  2.90662  - MSE:  45.3404191508  - Train Accuary:  0.665934\n",
      "epoch:  16  -  cost:  2.74846  - MSE:  43.2399216001  - Train Accuary:  0.661538\n",
      "epoch:  17  -  cost:  2.61366  - MSE:  41.5215071221  - Train Accuary:  0.657143\n",
      "epoch:  18  -  cost:  2.50051  - MSE:  40.1315245413  - Train Accuary:  0.659341\n",
      "epoch:  19  -  cost:  2.40693  - MSE:  39.0207414955  - Train Accuary:  0.648352\n",
      "epoch:  20  -  cost:  2.33003  - MSE:  38.139387126  - Train Accuary:  0.646154\n",
      "epoch:  21  -  cost:  2.26685  - MSE:  37.4419484777  - Train Accuary:  0.650549\n",
      "epoch:  22  -  cost:  2.21462  - MSE:  36.88951236  - Train Accuary:  0.643956\n",
      "epoch:  23  -  cost:  2.171  - MSE:  36.4512575241  - Train Accuary:  0.637363\n",
      "epoch:  24  -  cost:  2.13408  - MSE:  36.1004734588  - Train Accuary:  0.646154\n",
      "epoch:  25  -  cost:  2.10229  - MSE:  35.8169070538  - Train Accuary:  0.648352\n",
      "epoch:  26  -  cost:  2.07436  - MSE:  35.5853709504  - Train Accuary:  0.650549\n",
      "epoch:  27  -  cost:  2.04939  - MSE:  35.3939187886  - Train Accuary:  0.650549\n",
      "epoch:  28  -  cost:  2.02674  - MSE:  35.2337314924  - Train Accuary:  0.652747\n",
      "epoch:  29  -  cost:  2.00588  - MSE:  35.0971418413  - Train Accuary:  0.652747\n",
      "epoch:  30  -  cost:  1.98642  - MSE:  34.9790295851  - Train Accuary:  0.652747\n",
      "epoch:  31  -  cost:  1.96807  - MSE:  34.8755782958  - Train Accuary:  0.650549\n",
      "epoch:  32  -  cost:  1.95064  - MSE:  34.7829212092  - Train Accuary:  0.650549\n",
      "epoch:  33  -  cost:  1.93387  - MSE:  34.6980571568  - Train Accuary:  0.657143\n",
      "epoch:  34  -  cost:  1.91767  - MSE:  34.6207672146  - Train Accuary:  0.657143\n",
      "epoch:  35  -  cost:  1.90196  - MSE:  34.5494049549  - Train Accuary:  0.659341\n",
      "epoch:  36  -  cost:  1.88665  - MSE:  34.4829152266  - Train Accuary:  0.663736\n",
      "epoch:  37  -  cost:  1.87168  - MSE:  34.4208208619  - Train Accuary:  0.665934\n",
      "epoch:  38  -  cost:  1.85699  - MSE:  34.3624413401  - Train Accuary:  0.665934\n",
      "epoch:  39  -  cost:  1.84254  - MSE:  34.3071268364  - Train Accuary:  0.668132\n",
      "epoch:  40  -  cost:  1.82827  - MSE:  34.2539941271  - Train Accuary:  0.67033\n",
      "epoch:  41  -  cost:  1.81423  - MSE:  34.20319741  - Train Accuary:  0.67033\n",
      "epoch:  42  -  cost:  1.80037  - MSE:  34.1555063122  - Train Accuary:  0.67033\n",
      "epoch:  43  -  cost:  1.78675  - MSE:  34.1103074501  - Train Accuary:  0.67033\n",
      "epoch:  44  -  cost:  1.77337  - MSE:  34.0668092014  - Train Accuary:  0.67033\n",
      "epoch:  45  -  cost:  1.76021  - MSE:  34.0250520122  - Train Accuary:  0.672527\n",
      "epoch:  46  -  cost:  1.74724  - MSE:  33.9852403049  - Train Accuary:  0.672527\n",
      "epoch:  47  -  cost:  1.7345  - MSE:  33.9461672099  - Train Accuary:  0.672527\n",
      "epoch:  48  -  cost:  1.72195  - MSE:  33.9091947328  - Train Accuary:  0.672527\n",
      "epoch:  49  -  cost:  1.70959  - MSE:  33.873640238  - Train Accuary:  0.674725\n",
      "epoch:  50  -  cost:  1.69742  - MSE:  33.8395730365  - Train Accuary:  0.679121\n",
      "epoch:  51  -  cost:  1.6854  - MSE:  33.8068204617  - Train Accuary:  0.681319\n",
      "epoch:  52  -  cost:  1.67355  - MSE:  33.7757411267  - Train Accuary:  0.679121\n",
      "epoch:  53  -  cost:  1.66189  - MSE:  33.745682605  - Train Accuary:  0.679121\n",
      "epoch:  54  -  cost:  1.65041  - MSE:  33.7170782349  - Train Accuary:  0.679121\n",
      "epoch:  55  -  cost:  1.63913  - MSE:  33.6890713512  - Train Accuary:  0.679121\n",
      "epoch:  56  -  cost:  1.62802  - MSE:  33.6621632816  - Train Accuary:  0.679121\n",
      "epoch:  57  -  cost:  1.61707  - MSE:  33.6364515361  - Train Accuary:  0.683517\n",
      "epoch:  58  -  cost:  1.60628  - MSE:  33.611717502  - Train Accuary:  0.687912\n",
      "epoch:  59  -  cost:  1.59566  - MSE:  33.5881103691  - Train Accuary:  0.692308\n",
      "epoch:  60  -  cost:  1.5852  - MSE:  33.5652685615  - Train Accuary:  0.694506\n",
      "epoch:  61  -  cost:  1.57493  - MSE:  33.5437273108  - Train Accuary:  0.696703\n",
      "epoch:  62  -  cost:  1.56482  - MSE:  33.5234520196  - Train Accuary:  0.696703\n",
      "epoch:  63  -  cost:  1.55487  - MSE:  33.5041704845  - Train Accuary:  0.698901\n",
      "epoch:  64  -  cost:  1.54506  - MSE:  33.4851332047  - Train Accuary:  0.701099\n",
      "epoch:  65  -  cost:  1.53537  - MSE:  33.4656406019  - Train Accuary:  0.701099\n",
      "epoch:  66  -  cost:  1.52584  - MSE:  33.4471507639  - Train Accuary:  0.703297\n",
      "epoch:  67  -  cost:  1.51644  - MSE:  33.4293806939  - Train Accuary:  0.703297\n",
      "epoch:  68  -  cost:  1.50719  - MSE:  33.4124004396  - Train Accuary:  0.705495\n",
      "epoch:  69  -  cost:  1.49808  - MSE:  33.3963411591  - Train Accuary:  0.70989\n",
      "epoch:  70  -  cost:  1.48905  - MSE:  33.3818515437  - Train Accuary:  0.70989\n",
      "epoch:  71  -  cost:  1.4801  - MSE:  33.3687792903  - Train Accuary:  0.712088\n",
      "epoch:  72  -  cost:  1.47121  - MSE:  33.355595225  - Train Accuary:  0.718681\n",
      "epoch:  73  -  cost:  1.46247  - MSE:  33.3433719601  - Train Accuary:  0.718681\n",
      "epoch:  74  -  cost:  1.45384  - MSE:  33.3319457678  - Train Accuary:  0.718681\n",
      "epoch:  75  -  cost:  1.44535  - MSE:  33.3211653003  - Train Accuary:  0.718681\n",
      "epoch:  76  -  cost:  1.43698  - MSE:  33.3109073417  - Train Accuary:  0.720879\n",
      "epoch:  77  -  cost:  1.42872  - MSE:  33.3011434939  - Train Accuary:  0.720879\n",
      "epoch:  78  -  cost:  1.42058  - MSE:  33.2920603055  - Train Accuary:  0.723077\n",
      "epoch:  79  -  cost:  1.41256  - MSE:  33.2835186392  - Train Accuary:  0.725275\n",
      "epoch:  80  -  cost:  1.40466  - MSE:  33.2754859938  - Train Accuary:  0.725275\n",
      "epoch:  81  -  cost:  1.39686  - MSE:  33.267207846  - Train Accuary:  0.72967\n",
      "epoch:  82  -  cost:  1.38918  - MSE:  33.2595083092  - Train Accuary:  0.72967\n",
      "epoch:  83  -  cost:  1.3816  - MSE:  33.2523798156  - Train Accuary:  0.731868\n",
      "epoch:  84  -  cost:  1.37412  - MSE:  33.2457381011  - Train Accuary:  0.731868\n",
      "epoch:  85  -  cost:  1.36673  - MSE:  33.2396011053  - Train Accuary:  0.731868\n",
      "epoch:  86  -  cost:  1.35945  - MSE:  33.2342825409  - Train Accuary:  0.734066\n",
      "epoch:  87  -  cost:  1.35226  - MSE:  33.229528552  - Train Accuary:  0.734066\n",
      "epoch:  88  -  cost:  1.34518  - MSE:  33.2252874703  - Train Accuary:  0.736264\n",
      "epoch:  89  -  cost:  1.33819  - MSE:  33.221478904  - Train Accuary:  0.742857\n",
      "epoch:  90  -  cost:  1.3313  - MSE:  33.2181319477  - Train Accuary:  0.742857\n",
      "epoch:  91  -  cost:  1.32453  - MSE:  33.2156325313  - Train Accuary:  0.745055\n",
      "epoch:  92  -  cost:  1.31786  - MSE:  33.2135047852  - Train Accuary:  0.747253\n",
      "epoch:  93  -  cost:  1.31127  - MSE:  33.211889543  - Train Accuary:  0.747253\n",
      "epoch:  94  -  cost:  1.30477  - MSE:  33.2107230249  - Train Accuary:  0.749451\n",
      "epoch:  95  -  cost:  1.29836  - MSE:  33.210477575  - Train Accuary:  0.756044\n",
      "epoch:  96  -  cost:  1.29203  - MSE:  33.2107110415  - Train Accuary:  0.756044\n",
      "epoch:  97  -  cost:  1.28577  - MSE:  33.2119574341  - Train Accuary:  0.758242\n",
      "epoch:  98  -  cost:  1.2796  - MSE:  33.2138114978  - Train Accuary:  0.758242\n",
      "epoch:  99  -  cost:  1.2735  - MSE:  33.2161953366  - Train Accuary:  0.758242\n",
      "epoch:  100  -  cost:  1.26748  - MSE:  33.2191325337  - Train Accuary:  0.76044\n",
      "epoch:  101  -  cost:  1.26153  - MSE:  33.222270469  - Train Accuary:  0.76044\n",
      "epoch:  102  -  cost:  1.25564  - MSE:  33.2256346034  - Train Accuary:  0.76044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  103  -  cost:  1.24982  - MSE:  33.2294965066  - Train Accuary:  0.76044\n",
      "epoch:  104  -  cost:  1.24405  - MSE:  33.2339537989  - Train Accuary:  0.762637\n",
      "epoch:  105  -  cost:  1.23835  - MSE:  33.2385701465  - Train Accuary:  0.762637\n",
      "epoch:  106  -  cost:  1.23271  - MSE:  33.243514517  - Train Accuary:  0.762637\n",
      "epoch:  107  -  cost:  1.22713  - MSE:  33.2488068661  - Train Accuary:  0.762637\n",
      "epoch:  108  -  cost:  1.22161  - MSE:  33.2543264063  - Train Accuary:  0.767033\n",
      "epoch:  109  -  cost:  1.21616  - MSE:  33.260137185  - Train Accuary:  0.769231\n",
      "epoch:  110  -  cost:  1.21076  - MSE:  33.2662225056  - Train Accuary:  0.769231\n",
      "epoch:  111  -  cost:  1.20541  - MSE:  33.2727704702  - Train Accuary:  0.767033\n",
      "epoch:  112  -  cost:  1.20012  - MSE:  33.2795855509  - Train Accuary:  0.769231\n",
      "epoch:  113  -  cost:  1.19488  - MSE:  33.2865856518  - Train Accuary:  0.769231\n",
      "epoch:  114  -  cost:  1.1897  - MSE:  33.2938479123  - Train Accuary:  0.769231\n",
      "epoch:  115  -  cost:  1.18457  - MSE:  33.3013588195  - Train Accuary:  0.769231\n",
      "epoch:  116  -  cost:  1.17951  - MSE:  33.3091120684  - Train Accuary:  0.769231\n",
      "epoch:  117  -  cost:  1.17452  - MSE:  33.316570691  - Train Accuary:  0.771429\n",
      "epoch:  118  -  cost:  1.16958  - MSE:  33.3241494517  - Train Accuary:  0.773626\n",
      "epoch:  119  -  cost:  1.16469  - MSE:  33.3317715416  - Train Accuary:  0.773626\n",
      "epoch:  120  -  cost:  1.15984  - MSE:  33.3396068835  - Train Accuary:  0.773626\n",
      "epoch:  121  -  cost:  1.15506  - MSE:  33.3476757186  - Train Accuary:  0.773626\n",
      "epoch:  122  -  cost:  1.15031  - MSE:  33.3557561306  - Train Accuary:  0.773626\n",
      "epoch:  123  -  cost:  1.14561  - MSE:  33.3640283927  - Train Accuary:  0.775824\n",
      "epoch:  124  -  cost:  1.14095  - MSE:  33.3725964806  - Train Accuary:  0.775824\n",
      "epoch:  125  -  cost:  1.13632  - MSE:  33.3815919842  - Train Accuary:  0.775824\n",
      "epoch:  126  -  cost:  1.13174  - MSE:  33.3908822687  - Train Accuary:  0.775824\n",
      "epoch:  127  -  cost:  1.12719  - MSE:  33.4003611646  - Train Accuary:  0.775824\n",
      "epoch:  128  -  cost:  1.12266  - MSE:  33.4103336698  - Train Accuary:  0.778022\n",
      "epoch:  129  -  cost:  1.11818  - MSE:  33.4203209013  - Train Accuary:  0.78022\n",
      "epoch:  130  -  cost:  1.11374  - MSE:  33.4305763455  - Train Accuary:  0.782418\n",
      "epoch:  131  -  cost:  1.10934  - MSE:  33.4409394218  - Train Accuary:  0.782418\n",
      "epoch:  132  -  cost:  1.10498  - MSE:  33.451474788  - Train Accuary:  0.782418\n",
      "epoch:  133  -  cost:  1.10066  - MSE:  33.4620074147  - Train Accuary:  0.782418\n",
      "epoch:  134  -  cost:  1.09636  - MSE:  33.4725816913  - Train Accuary:  0.782418\n",
      "epoch:  135  -  cost:  1.09209  - MSE:  33.4834789828  - Train Accuary:  0.782418\n",
      "epoch:  136  -  cost:  1.08785  - MSE:  33.4944814388  - Train Accuary:  0.782418\n",
      "epoch:  137  -  cost:  1.08366  - MSE:  33.5056733022  - Train Accuary:  0.782418\n",
      "epoch:  138  -  cost:  1.0795  - MSE:  33.5171403273  - Train Accuary:  0.782418\n",
      "epoch:  139  -  cost:  1.0754  - MSE:  33.528360084  - Train Accuary:  0.782418\n",
      "epoch:  140  -  cost:  1.07133  - MSE:  33.5396752534  - Train Accuary:  0.782418\n",
      "epoch:  141  -  cost:  1.0673  - MSE:  33.5511481992  - Train Accuary:  0.782418\n",
      "epoch:  142  -  cost:  1.0633  - MSE:  33.5628468545  - Train Accuary:  0.782418\n",
      "epoch:  143  -  cost:  1.05934  - MSE:  33.5748263716  - Train Accuary:  0.782418\n",
      "epoch:  144  -  cost:  1.05541  - MSE:  33.5868898492  - Train Accuary:  0.782418\n",
      "epoch:  145  -  cost:  1.05152  - MSE:  33.5990452702  - Train Accuary:  0.782418\n",
      "epoch:  146  -  cost:  1.04766  - MSE:  33.611321314  - Train Accuary:  0.782418\n",
      "epoch:  147  -  cost:  1.04383  - MSE:  33.6239245235  - Train Accuary:  0.782418\n",
      "epoch:  148  -  cost:  1.04003  - MSE:  33.6365418152  - Train Accuary:  0.784615\n",
      "epoch:  149  -  cost:  1.03626  - MSE:  33.6489894198  - Train Accuary:  0.784615\n",
      "epoch:  150  -  cost:  1.0325  - MSE:  33.6612946551  - Train Accuary:  0.784615\n",
      "epoch:  151  -  cost:  1.02878  - MSE:  33.6737338855  - Train Accuary:  0.784615\n",
      "epoch:  152  -  cost:  1.02512  - MSE:  33.6863009969  - Train Accuary:  0.784615\n",
      "epoch:  153  -  cost:  1.02149  - MSE:  33.6989763468  - Train Accuary:  0.784615\n",
      "epoch:  154  -  cost:  1.01788  - MSE:  33.7117157132  - Train Accuary:  0.786813\n",
      "epoch:  155  -  cost:  1.0143  - MSE:  33.7248092752  - Train Accuary:  0.786813\n",
      "epoch:  156  -  cost:  1.01076  - MSE:  33.7377226891  - Train Accuary:  0.786813\n",
      "epoch:  157  -  cost:  1.00725  - MSE:  33.7507190409  - Train Accuary:  0.789011\n",
      "epoch:  158  -  cost:  1.00377  - MSE:  33.7638109379  - Train Accuary:  0.789011\n",
      "epoch:  159  -  cost:  1.00035  - MSE:  33.7762965327  - Train Accuary:  0.791209\n",
      "epoch:  160  -  cost:  0.996954  - MSE:  33.7887898185  - Train Accuary:  0.791209\n",
      "epoch:  161  -  cost:  0.993584  - MSE:  33.8013756842  - Train Accuary:  0.791209\n",
      "epoch:  162  -  cost:  0.990237  - MSE:  33.814044374  - Train Accuary:  0.791209\n",
      "epoch:  163  -  cost:  0.986917  - MSE:  33.8267558429  - Train Accuary:  0.791209\n",
      "epoch:  164  -  cost:  0.983623  - MSE:  33.8395382036  - Train Accuary:  0.791209\n",
      "epoch:  165  -  cost:  0.980349  - MSE:  33.8523990953  - Train Accuary:  0.791209\n",
      "epoch:  166  -  cost:  0.9771  - MSE:  33.8652878165  - Train Accuary:  0.793407\n",
      "epoch:  167  -  cost:  0.973877  - MSE:  33.8779916052  - Train Accuary:  0.793407\n",
      "epoch:  168  -  cost:  0.970678  - MSE:  33.8905693788  - Train Accuary:  0.795604\n",
      "epoch:  169  -  cost:  0.967504  - MSE:  33.9031093373  - Train Accuary:  0.795604\n",
      "epoch:  170  -  cost:  0.964354  - MSE:  33.915630213  - Train Accuary:  0.795604\n",
      "epoch:  171  -  cost:  0.961227  - MSE:  33.928084861  - Train Accuary:  0.795604\n",
      "epoch:  172  -  cost:  0.958125  - MSE:  33.9406064021  - Train Accuary:  0.795604\n",
      "epoch:  173  -  cost:  0.955049  - MSE:  33.9531160114  - Train Accuary:  0.8\n",
      "epoch:  174  -  cost:  0.951997  - MSE:  33.9657004577  - Train Accuary:  0.8\n",
      "epoch:  175  -  cost:  0.948967  - MSE:  33.9783277606  - Train Accuary:  0.8\n",
      "epoch:  176  -  cost:  0.94596  - MSE:  33.9909202079  - Train Accuary:  0.802198\n",
      "epoch:  177  -  cost:  0.942975  - MSE:  34.0034900057  - Train Accuary:  0.802198\n",
      "epoch:  178  -  cost:  0.940012  - MSE:  34.0160821519  - Train Accuary:  0.802198\n",
      "epoch:  179  -  cost:  0.937072  - MSE:  34.0287258397  - Train Accuary:  0.802198\n",
      "epoch:  180  -  cost:  0.934153  - MSE:  34.0413928635  - Train Accuary:  0.802198\n",
      "epoch:  181  -  cost:  0.931263  - MSE:  34.0540807156  - Train Accuary:  0.802198\n",
      "epoch:  182  -  cost:  0.928408  - MSE:  34.066546767  - Train Accuary:  0.802198\n",
      "epoch:  183  -  cost:  0.925574  - MSE:  34.0790335189  - Train Accuary:  0.804396\n",
      "epoch:  184  -  cost:  0.92276  - MSE:  34.0915580172  - Train Accuary:  0.804396\n",
      "epoch:  185  -  cost:  0.919968  - MSE:  34.1040813048  - Train Accuary:  0.804396\n",
      "epoch:  186  -  cost:  0.917195  - MSE:  34.116586391  - Train Accuary:  0.806593\n",
      "epoch:  187  -  cost:  0.914444  - MSE:  34.1291126858  - Train Accuary:  0.806593\n",
      "epoch:  188  -  cost:  0.911712  - MSE:  34.1416770609  - Train Accuary:  0.806593\n",
      "epoch:  189  -  cost:  0.909001  - MSE:  34.1542652878  - Train Accuary:  0.806593\n",
      "epoch:  190  -  cost:  0.906312  - MSE:  34.1669250603  - Train Accuary:  0.806593\n",
      "epoch:  191  -  cost:  0.903646  - MSE:  34.1796305984  - Train Accuary:  0.806593\n",
      "epoch:  192  -  cost:  0.900999  - MSE:  34.1923201873  - Train Accuary:  0.806593\n",
      "epoch:  193  -  cost:  0.898371  - MSE:  34.2049294984  - Train Accuary:  0.804396\n",
      "epoch:  194  -  cost:  0.89576  - MSE:  34.2176549626  - Train Accuary:  0.804396\n",
      "epoch:  195  -  cost:  0.89316  - MSE:  34.2301987609  - Train Accuary:  0.804396\n",
      "epoch:  196  -  cost:  0.890574  - MSE:  34.2426919323  - Train Accuary:  0.804396\n",
      "epoch:  197  -  cost:  0.888007  - MSE:  34.2551914449  - Train Accuary:  0.804396\n",
      "epoch:  198  -  cost:  0.885458  - MSE:  34.267704813  - Train Accuary:  0.806593\n",
      "epoch:  199  -  cost:  0.882926  - MSE:  34.280220518  - Train Accuary:  0.806593\n",
      "epoch:  200  -  cost:  0.880412  - MSE:  34.2927608793  - Train Accuary:  0.806593\n",
      "epoch:  201  -  cost:  0.877915  - MSE:  34.3052790829  - Train Accuary:  0.808791\n",
      "epoch:  202  -  cost:  0.875432  - MSE:  34.3178431466  - Train Accuary:  0.808791\n",
      "epoch:  203  -  cost:  0.872951  - MSE:  34.3303045477  - Train Accuary:  0.808791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  204  -  cost:  0.870489  - MSE:  34.342830356  - Train Accuary:  0.808791\n",
      "epoch:  205  -  cost:  0.868042  - MSE:  34.3553552117  - Train Accuary:  0.808791\n",
      "epoch:  206  -  cost:  0.865612  - MSE:  34.3678407157  - Train Accuary:  0.808791\n",
      "epoch:  207  -  cost:  0.863195  - MSE:  34.3803187491  - Train Accuary:  0.808791\n",
      "epoch:  208  -  cost:  0.860779  - MSE:  34.3925060791  - Train Accuary:  0.810989\n",
      "epoch:  209  -  cost:  0.858373  - MSE:  34.4049827847  - Train Accuary:  0.810989\n",
      "epoch:  210  -  cost:  0.855967  - MSE:  34.4177545645  - Train Accuary:  0.810989\n",
      "epoch:  211  -  cost:  0.853575  - MSE:  34.43052968  - Train Accuary:  0.813187\n",
      "epoch:  212  -  cost:  0.851199  - MSE:  34.443146171  - Train Accuary:  0.815385\n",
      "epoch:  213  -  cost:  0.848838  - MSE:  34.4554415997  - Train Accuary:  0.815385\n",
      "epoch:  214  -  cost:  0.84649  - MSE:  34.467749863  - Train Accuary:  0.815385\n",
      "epoch:  215  -  cost:  0.844139  - MSE:  34.4796700612  - Train Accuary:  0.817582\n",
      "epoch:  216  -  cost:  0.841801  - MSE:  34.4917936081  - Train Accuary:  0.817582\n",
      "epoch:  217  -  cost:  0.839478  - MSE:  34.5038820979  - Train Accuary:  0.81978\n",
      "epoch:  218  -  cost:  0.83717  - MSE:  34.5159959854  - Train Accuary:  0.81978\n",
      "epoch:  219  -  cost:  0.834877  - MSE:  34.5280758634  - Train Accuary:  0.81978\n",
      "epoch:  220  -  cost:  0.832597  - MSE:  34.5401505394  - Train Accuary:  0.81978\n",
      "epoch:  221  -  cost:  0.830333  - MSE:  34.5521233148  - Train Accuary:  0.821978\n",
      "epoch:  222  -  cost:  0.828082  - MSE:  34.5639988017  - Train Accuary:  0.821978\n",
      "epoch:  223  -  cost:  0.825841  - MSE:  34.5759280868  - Train Accuary:  0.821978\n",
      "epoch:  224  -  cost:  0.823599  - MSE:  34.5874977406  - Train Accuary:  0.821978\n",
      "epoch:  225  -  cost:  0.821371  - MSE:  34.5989397253  - Train Accuary:  0.821978\n",
      "epoch:  226  -  cost:  0.819156  - MSE:  34.6103272206  - Train Accuary:  0.821978\n",
      "epoch:  227  -  cost:  0.816956  - MSE:  34.6216474168  - Train Accuary:  0.821978\n",
      "epoch:  228  -  cost:  0.814768  - MSE:  34.6329254961  - Train Accuary:  0.821978\n",
      "epoch:  229  -  cost:  0.812594  - MSE:  34.6442031359  - Train Accuary:  0.821978\n",
      "epoch:  230  -  cost:  0.810429  - MSE:  34.6551802245  - Train Accuary:  0.821978\n",
      "epoch:  231  -  cost:  0.808279  - MSE:  34.6661600314  - Train Accuary:  0.821978\n",
      "epoch:  232  -  cost:  0.806144  - MSE:  34.6771295366  - Train Accuary:  0.821978\n",
      "epoch:  233  -  cost:  0.804022  - MSE:  34.6880731364  - Train Accuary:  0.821978\n",
      "epoch:  234  -  cost:  0.801916  - MSE:  34.6990295953  - Train Accuary:  0.821978\n",
      "epoch:  235  -  cost:  0.799834  - MSE:  34.7098964422  - Train Accuary:  0.821978\n",
      "epoch:  236  -  cost:  0.797764  - MSE:  34.7207927478  - Train Accuary:  0.821978\n",
      "epoch:  237  -  cost:  0.795706  - MSE:  34.7316402735  - Train Accuary:  0.821978\n",
      "epoch:  238  -  cost:  0.793659  - MSE:  34.742484535  - Train Accuary:  0.821978\n",
      "epoch:  239  -  cost:  0.791624  - MSE:  34.7533097485  - Train Accuary:  0.821978\n",
      "epoch:  240  -  cost:  0.789601  - MSE:  34.7641416921  - Train Accuary:  0.824176\n",
      "epoch:  241  -  cost:  0.78759  - MSE:  34.7749830499  - Train Accuary:  0.826374\n",
      "epoch:  242  -  cost:  0.785589  - MSE:  34.7858278396  - Train Accuary:  0.826374\n",
      "epoch:  243  -  cost:  0.783606  - MSE:  34.7966589308  - Train Accuary:  0.826374\n",
      "epoch:  244  -  cost:  0.781644  - MSE:  34.8071565045  - Train Accuary:  0.826374\n",
      "epoch:  245  -  cost:  0.779691  - MSE:  34.8177817987  - Train Accuary:  0.826374\n",
      "epoch:  246  -  cost:  0.777749  - MSE:  34.8283882199  - Train Accuary:  0.826374\n",
      "epoch:  247  -  cost:  0.775811  - MSE:  34.8389948752  - Train Accuary:  0.826374\n",
      "epoch:  248  -  cost:  0.77387  - MSE:  34.849381475  - Train Accuary:  0.826374\n",
      "epoch:  249  -  cost:  0.77194  - MSE:  34.8597889789  - Train Accuary:  0.826374\n",
      "epoch:  250  -  cost:  0.77002  - MSE:  34.8702967262  - Train Accuary:  0.826374\n",
      "epoch:  251  -  cost:  0.768111  - MSE:  34.880787706  - Train Accuary:  0.828571\n",
      "epoch:  252  -  cost:  0.766213  - MSE:  34.8912816342  - Train Accuary:  0.830769\n",
      "epoch:  253  -  cost:  0.764323  - MSE:  34.9017519699  - Train Accuary:  0.830769\n",
      "epoch:  254  -  cost:  0.762428  - MSE:  34.9124092647  - Train Accuary:  0.830769\n",
      "epoch:  255  -  cost:  0.760543  - MSE:  34.9229742903  - Train Accuary:  0.830769\n",
      "epoch:  256  -  cost:  0.758668  - MSE:  34.9335300598  - Train Accuary:  0.830769\n",
      "epoch:  257  -  cost:  0.756803  - MSE:  34.9440806653  - Train Accuary:  0.830769\n",
      "epoch:  258  -  cost:  0.754947  - MSE:  34.954620805  - Train Accuary:  0.830769\n",
      "epoch:  259  -  cost:  0.753101  - MSE:  34.9651478619  - Train Accuary:  0.830769\n",
      "epoch:  260  -  cost:  0.751266  - MSE:  34.9755844698  - Train Accuary:  0.830769\n",
      "epoch:  261  -  cost:  0.749451  - MSE:  34.9857850525  - Train Accuary:  0.830769\n",
      "epoch:  262  -  cost:  0.747646  - MSE:  34.9959724974  - Train Accuary:  0.830769\n",
      "epoch:  263  -  cost:  0.745849  - MSE:  35.0061204255  - Train Accuary:  0.830769\n",
      "epoch:  264  -  cost:  0.744062  - MSE:  35.0161993671  - Train Accuary:  0.832967\n",
      "epoch:  265  -  cost:  0.742283  - MSE:  35.0262610153  - Train Accuary:  0.832967\n",
      "epoch:  266  -  cost:  0.740514  - MSE:  35.0363508989  - Train Accuary:  0.832967\n",
      "epoch:  267  -  cost:  0.738754  - MSE:  35.0464464056  - Train Accuary:  0.832967\n",
      "epoch:  268  -  cost:  0.737002  - MSE:  35.0565126596  - Train Accuary:  0.832967\n",
      "epoch:  269  -  cost:  0.735261  - MSE:  35.0664051385  - Train Accuary:  0.832967\n",
      "epoch:  270  -  cost:  0.733529  - MSE:  35.0763024123  - Train Accuary:  0.835165\n",
      "epoch:  271  -  cost:  0.731806  - MSE:  35.0861692679  - Train Accuary:  0.835165\n",
      "epoch:  272  -  cost:  0.730091  - MSE:  35.0960215564  - Train Accuary:  0.835165\n",
      "epoch:  273  -  cost:  0.728391  - MSE:  35.1058285484  - Train Accuary:  0.835165\n",
      "epoch:  274  -  cost:  0.726703  - MSE:  35.1155100653  - Train Accuary:  0.835165\n",
      "epoch:  275  -  cost:  0.725024  - MSE:  35.125201774  - Train Accuary:  0.835165\n",
      "epoch:  276  -  cost:  0.723354  - MSE:  35.1349138115  - Train Accuary:  0.835165\n",
      "epoch:  277  -  cost:  0.721691  - MSE:  35.1446269365  - Train Accuary:  0.835165\n",
      "epoch:  278  -  cost:  0.720036  - MSE:  35.1543131163  - Train Accuary:  0.837363\n",
      "epoch:  279  -  cost:  0.718388  - MSE:  35.163946169  - Train Accuary:  0.837363\n",
      "epoch:  280  -  cost:  0.71675  - MSE:  35.1735716588  - Train Accuary:  0.837363\n",
      "epoch:  281  -  cost:  0.715118  - MSE:  35.1831776162  - Train Accuary:  0.837363\n",
      "epoch:  282  -  cost:  0.713496  - MSE:  35.1927003133  - Train Accuary:  0.837363\n",
      "epoch:  283  -  cost:  0.711885  - MSE:  35.2020526167  - Train Accuary:  0.837363\n",
      "epoch:  284  -  cost:  0.710281  - MSE:  35.211400681  - Train Accuary:  0.837363\n",
      "epoch:  285  -  cost:  0.708684  - MSE:  35.2207425708  - Train Accuary:  0.837363\n",
      "epoch:  286  -  cost:  0.707095  - MSE:  35.2300816118  - Train Accuary:  0.837363\n",
      "epoch:  287  -  cost:  0.705513  - MSE:  35.2394538282  - Train Accuary:  0.837363\n",
      "epoch:  288  -  cost:  0.703938  - MSE:  35.248797127  - Train Accuary:  0.837363\n",
      "epoch:  289  -  cost:  0.70237  - MSE:  35.2581523907  - Train Accuary:  0.837363\n",
      "epoch:  290  -  cost:  0.700809  - MSE:  35.2674883877  - Train Accuary:  0.83956\n",
      "epoch:  291  -  cost:  0.699256  - MSE:  35.2768298792  - Train Accuary:  0.83956\n",
      "epoch:  292  -  cost:  0.697709  - MSE:  35.286155318  - Train Accuary:  0.83956\n",
      "epoch:  293  -  cost:  0.696169  - MSE:  35.2954829984  - Train Accuary:  0.83956\n",
      "epoch:  294  -  cost:  0.694637  - MSE:  35.3047967732  - Train Accuary:  0.83956\n",
      "epoch:  295  -  cost:  0.693111  - MSE:  35.3141031755  - Train Accuary:  0.83956\n",
      "epoch:  296  -  cost:  0.691592  - MSE:  35.3234232167  - Train Accuary:  0.83956\n",
      "epoch:  297  -  cost:  0.690077  - MSE:  35.3327085641  - Train Accuary:  0.83956\n",
      "epoch:  298  -  cost:  0.688571  - MSE:  35.3419858879  - Train Accuary:  0.83956\n",
      "epoch:  299  -  cost:  0.68707  - MSE:  35.3512795461  - Train Accuary:  0.83956\n",
      "epoch:  300  -  cost:  0.685577  - MSE:  35.3605571488  - Train Accuary:  0.841758\n",
      "epoch:  301  -  cost:  0.68409  - MSE:  35.3698497932  - Train Accuary:  0.841758\n",
      "epoch:  302  -  cost:  0.682609  - MSE:  35.3791215057  - Train Accuary:  0.843956\n",
      "epoch:  303  -  cost:  0.681135  - MSE:  35.3883939033  - Train Accuary:  0.843956\n",
      "epoch:  304  -  cost:  0.679667  - MSE:  35.3976642879  - Train Accuary:  0.843956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  305  -  cost:  0.678206  - MSE:  35.4069302338  - Train Accuary:  0.843956\n",
      "epoch:  306  -  cost:  0.676751  - MSE:  35.4161814702  - Train Accuary:  0.843956\n",
      "epoch:  307  -  cost:  0.675303  - MSE:  35.4253837942  - Train Accuary:  0.846154\n",
      "epoch:  308  -  cost:  0.67386  - MSE:  35.4345724966  - Train Accuary:  0.846154\n",
      "epoch:  309  -  cost:  0.672424  - MSE:  35.4437630465  - Train Accuary:  0.846154\n",
      "epoch:  310  -  cost:  0.670996  - MSE:  35.4529323128  - Train Accuary:  0.846154\n",
      "epoch:  311  -  cost:  0.669573  - MSE:  35.4620990487  - Train Accuary:  0.846154\n",
      "epoch:  312  -  cost:  0.668154  - MSE:  35.4712571641  - Train Accuary:  0.846154\n",
      "epoch:  313  -  cost:  0.66674  - MSE:  35.4803660674  - Train Accuary:  0.846154\n",
      "epoch:  314  -  cost:  0.665334  - MSE:  35.4894185928  - Train Accuary:  0.846154\n",
      "epoch:  315  -  cost:  0.663934  - MSE:  35.4984651638  - Train Accuary:  0.843956\n",
      "epoch:  316  -  cost:  0.66254  - MSE:  35.5075154694  - Train Accuary:  0.843956\n",
      "epoch:  317  -  cost:  0.661153  - MSE:  35.5165383866  - Train Accuary:  0.843956\n",
      "epoch:  318  -  cost:  0.659772  - MSE:  35.5255493838  - Train Accuary:  0.843956\n",
      "epoch:  319  -  cost:  0.658396  - MSE:  35.5344969565  - Train Accuary:  0.843956\n",
      "epoch:  320  -  cost:  0.657026  - MSE:  35.5434130304  - Train Accuary:  0.843956\n",
      "epoch:  321  -  cost:  0.655661  - MSE:  35.552312916  - Train Accuary:  0.843956\n",
      "epoch:  322  -  cost:  0.654302  - MSE:  35.561219526  - Train Accuary:  0.843956\n",
      "epoch:  323  -  cost:  0.652949  - MSE:  35.5701210443  - Train Accuary:  0.843956\n",
      "epoch:  324  -  cost:  0.651601  - MSE:  35.579031225  - Train Accuary:  0.843956\n",
      "epoch:  325  -  cost:  0.650254  - MSE:  35.5879153349  - Train Accuary:  0.846154\n",
      "epoch:  326  -  cost:  0.648906  - MSE:  35.5966597258  - Train Accuary:  0.846154\n",
      "epoch:  327  -  cost:  0.647563  - MSE:  35.6053829435  - Train Accuary:  0.846154\n",
      "epoch:  328  -  cost:  0.646226  - MSE:  35.6140974709  - Train Accuary:  0.848352\n",
      "epoch:  329  -  cost:  0.644894  - MSE:  35.6227849199  - Train Accuary:  0.848352\n",
      "epoch:  330  -  cost:  0.643568  - MSE:  35.6314558801  - Train Accuary:  0.848352\n",
      "epoch:  331  -  cost:  0.642247  - MSE:  35.640008865  - Train Accuary:  0.848352\n",
      "epoch:  332  -  cost:  0.640932  - MSE:  35.6485227132  - Train Accuary:  0.850549\n",
      "epoch:  333  -  cost:  0.639622  - MSE:  35.6569730332  - Train Accuary:  0.850549\n",
      "epoch:  334  -  cost:  0.638317  - MSE:  35.6654182256  - Train Accuary:  0.850549\n",
      "epoch:  335  -  cost:  0.637018  - MSE:  35.6738395152  - Train Accuary:  0.850549\n",
      "epoch:  336  -  cost:  0.635724  - MSE:  35.6822543473  - Train Accuary:  0.850549\n",
      "epoch:  337  -  cost:  0.634435  - MSE:  35.6906536581  - Train Accuary:  0.850549\n",
      "epoch:  338  -  cost:  0.633155  - MSE:  35.6988158267  - Train Accuary:  0.850549\n",
      "epoch:  339  -  cost:  0.631874  - MSE:  35.7068652808  - Train Accuary:  0.850549\n",
      "epoch:  340  -  cost:  0.630599  - MSE:  35.7148924009  - Train Accuary:  0.850549\n",
      "epoch:  341  -  cost:  0.629328  - MSE:  35.7229522181  - Train Accuary:  0.850549\n",
      "epoch:  342  -  cost:  0.628063  - MSE:  35.7309882084  - Train Accuary:  0.852747\n",
      "epoch:  343  -  cost:  0.626803  - MSE:  35.7389767001  - Train Accuary:  0.852747\n",
      "epoch:  344  -  cost:  0.625551  - MSE:  35.7469549633  - Train Accuary:  0.857143\n",
      "epoch:  345  -  cost:  0.624309  - MSE:  35.7545269873  - Train Accuary:  0.857143\n",
      "epoch:  346  -  cost:  0.623073  - MSE:  35.7621037356  - Train Accuary:  0.857143\n",
      "epoch:  347  -  cost:  0.621842  - MSE:  35.7696542505  - Train Accuary:  0.857143\n",
      "epoch:  348  -  cost:  0.620615  - MSE:  35.7773005169  - Train Accuary:  0.857143\n",
      "epoch:  349  -  cost:  0.619393  - MSE:  35.7849374245  - Train Accuary:  0.857143\n",
      "epoch:  350  -  cost:  0.618176  - MSE:  35.7925801004  - Train Accuary:  0.857143\n",
      "epoch:  351  -  cost:  0.616964  - MSE:  35.8002005889  - Train Accuary:  0.857143\n",
      "epoch:  352  -  cost:  0.615757  - MSE:  35.8077949321  - Train Accuary:  0.857143\n",
      "epoch:  353  -  cost:  0.614555  - MSE:  35.8153606686  - Train Accuary:  0.857143\n",
      "epoch:  354  -  cost:  0.613357  - MSE:  35.8229285679  - Train Accuary:  0.857143\n",
      "epoch:  355  -  cost:  0.612164  - MSE:  35.8304728322  - Train Accuary:  0.857143\n",
      "epoch:  356  -  cost:  0.610975  - MSE:  35.8380221607  - Train Accuary:  0.857143\n",
      "epoch:  357  -  cost:  0.60979  - MSE:  35.8454879331  - Train Accuary:  0.857143\n",
      "epoch:  358  -  cost:  0.60861  - MSE:  35.8529426374  - Train Accuary:  0.857143\n",
      "epoch:  359  -  cost:  0.607434  - MSE:  35.8603934413  - Train Accuary:  0.857143\n",
      "epoch:  360  -  cost:  0.606263  - MSE:  35.8678178491  - Train Accuary:  0.857143\n",
      "epoch:  361  -  cost:  0.605094  - MSE:  35.875169083  - Train Accuary:  0.857143\n",
      "epoch:  362  -  cost:  0.603931  - MSE:  35.8824906172  - Train Accuary:  0.857143\n",
      "epoch:  363  -  cost:  0.602773  - MSE:  35.889812317  - Train Accuary:  0.857143\n",
      "epoch:  364  -  cost:  0.601621  - MSE:  35.8970280589  - Train Accuary:  0.859341\n",
      "epoch:  365  -  cost:  0.600473  - MSE:  35.9042442008  - Train Accuary:  0.859341\n",
      "epoch:  366  -  cost:  0.59933  - MSE:  35.9114419204  - Train Accuary:  0.859341\n",
      "epoch:  367  -  cost:  0.598191  - MSE:  35.9186108494  - Train Accuary:  0.859341\n",
      "epoch:  368  -  cost:  0.597058  - MSE:  35.9257717579  - Train Accuary:  0.859341\n",
      "epoch:  369  -  cost:  0.595929  - MSE:  35.9329676726  - Train Accuary:  0.859341\n",
      "epoch:  370  -  cost:  0.594806  - MSE:  35.9401587665  - Train Accuary:  0.859341\n",
      "epoch:  371  -  cost:  0.593689  - MSE:  35.9474743437  - Train Accuary:  0.859341\n",
      "epoch:  372  -  cost:  0.592577  - MSE:  35.9547552053  - Train Accuary:  0.859341\n",
      "epoch:  373  -  cost:  0.591468  - MSE:  35.9620027929  - Train Accuary:  0.859341\n",
      "epoch:  374  -  cost:  0.590357  - MSE:  35.9691990259  - Train Accuary:  0.859341\n",
      "epoch:  375  -  cost:  0.58925  - MSE:  35.9764099565  - Train Accuary:  0.859341\n",
      "epoch:  376  -  cost:  0.588148  - MSE:  35.9836039096  - Train Accuary:  0.859341\n",
      "epoch:  377  -  cost:  0.587049  - MSE:  35.9908089698  - Train Accuary:  0.859341\n",
      "epoch:  378  -  cost:  0.585955  - MSE:  35.9980120365  - Train Accuary:  0.859341\n",
      "epoch:  379  -  cost:  0.584864  - MSE:  36.0052373507  - Train Accuary:  0.859341\n",
      "epoch:  380  -  cost:  0.58378  - MSE:  36.0124576997  - Train Accuary:  0.859341\n",
      "epoch:  381  -  cost:  0.582706  - MSE:  36.0194366572  - Train Accuary:  0.859341\n",
      "epoch:  382  -  cost:  0.581636  - MSE:  36.0264206424  - Train Accuary:  0.859341\n",
      "epoch:  383  -  cost:  0.58057  - MSE:  36.0334433458  - Train Accuary:  0.859341\n",
      "epoch:  384  -  cost:  0.579512  - MSE:  36.0403850228  - Train Accuary:  0.859341\n",
      "epoch:  385  -  cost:  0.578468  - MSE:  36.0471520001  - Train Accuary:  0.859341\n",
      "epoch:  386  -  cost:  0.577437  - MSE:  36.053297765  - Train Accuary:  0.859341\n",
      "epoch:  387  -  cost:  0.576411  - MSE:  36.0594217638  - Train Accuary:  0.859341\n",
      "epoch:  388  -  cost:  0.57539  - MSE:  36.0655383529  - Train Accuary:  0.859341\n",
      "epoch:  389  -  cost:  0.574378  - MSE:  36.0714309048  - Train Accuary:  0.859341\n",
      "epoch:  390  -  cost:  0.57337  - MSE:  36.07730512  - Train Accuary:  0.859341\n",
      "epoch:  391  -  cost:  0.572365  - MSE:  36.0831623822  - Train Accuary:  0.859341\n",
      "epoch:  392  -  cost:  0.571364  - MSE:  36.0890093014  - Train Accuary:  0.861538\n",
      "epoch:  393  -  cost:  0.570366  - MSE:  36.0948489814  - Train Accuary:  0.861538\n",
      "epoch:  394  -  cost:  0.569371  - MSE:  36.1006651763  - Train Accuary:  0.861538\n",
      "epoch:  395  -  cost:  0.568379  - MSE:  36.1064617408  - Train Accuary:  0.861538\n",
      "epoch:  396  -  cost:  0.567391  - MSE:  36.1122661624  - Train Accuary:  0.861538\n",
      "epoch:  397  -  cost:  0.566405  - MSE:  36.1180760229  - Train Accuary:  0.861538\n",
      "epoch:  398  -  cost:  0.565417  - MSE:  36.1238213362  - Train Accuary:  0.861538\n",
      "epoch:  399  -  cost:  0.564432  - MSE:  36.1295549686  - Train Accuary:  0.861538\n",
      "epoch:  400  -  cost:  0.563452  - MSE:  36.1352840669  - Train Accuary:  0.861538\n",
      "epoch:  401  -  cost:  0.562474  - MSE:  36.1410186824  - Train Accuary:  0.861538\n",
      "epoch:  402  -  cost:  0.5615  - MSE:  36.1467326879  - Train Accuary:  0.861538\n",
      "epoch:  403  -  cost:  0.560529  - MSE:  36.1524504102  - Train Accuary:  0.861538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  404  -  cost:  0.559563  - MSE:  36.1582103563  - Train Accuary:  0.861538\n",
      "epoch:  405  -  cost:  0.558601  - MSE:  36.1639454487  - Train Accuary:  0.861538\n",
      "epoch:  406  -  cost:  0.557643  - MSE:  36.1696588063  - Train Accuary:  0.861538\n",
      "epoch:  407  -  cost:  0.556692  - MSE:  36.1753204117  - Train Accuary:  0.861538\n",
      "epoch:  408  -  cost:  0.555745  - MSE:  36.1809723679  - Train Accuary:  0.861538\n",
      "epoch:  409  -  cost:  0.554802  - MSE:  36.1866302379  - Train Accuary:  0.863736\n",
      "epoch:  410  -  cost:  0.553861  - MSE:  36.1922865611  - Train Accuary:  0.863736\n",
      "epoch:  411  -  cost:  0.552924  - MSE:  36.1979532606  - Train Accuary:  0.863736\n",
      "epoch:  412  -  cost:  0.55199  - MSE:  36.2036042432  - Train Accuary:  0.863736\n",
      "epoch:  413  -  cost:  0.55106  - MSE:  36.2092738663  - Train Accuary:  0.865934\n",
      "epoch:  414  -  cost:  0.550134  - MSE:  36.2147546936  - Train Accuary:  0.865934\n",
      "epoch:  415  -  cost:  0.549211  - MSE:  36.2202346822  - Train Accuary:  0.865934\n",
      "epoch:  416  -  cost:  0.548292  - MSE:  36.2257223647  - Train Accuary:  0.865934\n",
      "epoch:  417  -  cost:  0.547376  - MSE:  36.2312047304  - Train Accuary:  0.865934\n",
      "epoch:  418  -  cost:  0.546463  - MSE:  36.2364962603  - Train Accuary:  0.865934\n",
      "epoch:  419  -  cost:  0.545553  - MSE:  36.2417885646  - Train Accuary:  0.865934\n",
      "epoch:  420  -  cost:  0.544646  - MSE:  36.247074178  - Train Accuary:  0.865934\n",
      "epoch:  421  -  cost:  0.543742  - MSE:  36.2523711017  - Train Accuary:  0.865934\n",
      "epoch:  422  -  cost:  0.542841  - MSE:  36.2576557442  - Train Accuary:  0.865934\n",
      "epoch:  423  -  cost:  0.541943  - MSE:  36.2629412298  - Train Accuary:  0.865934\n",
      "epoch:  424  -  cost:  0.541049  - MSE:  36.2682162354  - Train Accuary:  0.865934\n",
      "epoch:  425  -  cost:  0.540157  - MSE:  36.2734889765  - Train Accuary:  0.865934\n",
      "epoch:  426  -  cost:  0.539268  - MSE:  36.278750727  - Train Accuary:  0.865934\n",
      "epoch:  427  -  cost:  0.538383  - MSE:  36.284011983  - Train Accuary:  0.865934\n",
      "epoch:  428  -  cost:  0.537502  - MSE:  36.2892802789  - Train Accuary:  0.865934\n",
      "epoch:  429  -  cost:  0.53663  - MSE:  36.2943702383  - Train Accuary:  0.865934\n",
      "epoch:  430  -  cost:  0.535761  - MSE:  36.2994699282  - Train Accuary:  0.865934\n",
      "epoch:  431  -  cost:  0.534895  - MSE:  36.3046138732  - Train Accuary:  0.865934\n",
      "epoch:  432  -  cost:  0.534031  - MSE:  36.3097549577  - Train Accuary:  0.865934\n",
      "epoch:  433  -  cost:  0.533171  - MSE:  36.314895346  - Train Accuary:  0.865934\n",
      "epoch:  434  -  cost:  0.532313  - MSE:  36.3200305143  - Train Accuary:  0.865934\n",
      "epoch:  435  -  cost:  0.531459  - MSE:  36.3251547569  - Train Accuary:  0.865934\n",
      "epoch:  436  -  cost:  0.530608  - MSE:  36.3302773195  - Train Accuary:  0.865934\n",
      "epoch:  437  -  cost:  0.529761  - MSE:  36.3353188474  - Train Accuary:  0.865934\n",
      "epoch:  438  -  cost:  0.528917  - MSE:  36.3403541188  - Train Accuary:  0.865934\n",
      "epoch:  439  -  cost:  0.528076  - MSE:  36.345340121  - Train Accuary:  0.865934\n",
      "epoch:  440  -  cost:  0.527237  - MSE:  36.3503240561  - Train Accuary:  0.868132\n",
      "epoch:  441  -  cost:  0.526401  - MSE:  36.3552983972  - Train Accuary:  0.868132\n",
      "epoch:  442  -  cost:  0.525568  - MSE:  36.3602231569  - Train Accuary:  0.868132\n",
      "epoch:  443  -  cost:  0.524738  - MSE:  36.365173846  - Train Accuary:  0.868132\n",
      "epoch:  444  -  cost:  0.52391  - MSE:  36.3701086827  - Train Accuary:  0.868132\n",
      "epoch:  445  -  cost:  0.523085  - MSE:  36.3750507796  - Train Accuary:  0.868132\n",
      "epoch:  446  -  cost:  0.522263  - MSE:  36.3799684255  - Train Accuary:  0.868132\n",
      "epoch:  447  -  cost:  0.521444  - MSE:  36.3848903239  - Train Accuary:  0.868132\n",
      "epoch:  448  -  cost:  0.520627  - MSE:  36.3898037016  - Train Accuary:  0.868132\n",
      "epoch:  449  -  cost:  0.519814  - MSE:  36.3946533193  - Train Accuary:  0.868132\n",
      "epoch:  450  -  cost:  0.519004  - MSE:  36.3994913057  - Train Accuary:  0.868132\n",
      "epoch:  451  -  cost:  0.518197  - MSE:  36.4043193073  - Train Accuary:  0.868132\n",
      "epoch:  452  -  cost:  0.517391  - MSE:  36.4091604337  - Train Accuary:  0.868132\n",
      "epoch:  453  -  cost:  0.516589  - MSE:  36.413954149  - Train Accuary:  0.868132\n",
      "epoch:  454  -  cost:  0.515789  - MSE:  36.4187451189  - Train Accuary:  0.87033\n",
      "epoch:  455  -  cost:  0.514992  - MSE:  36.423476245  - Train Accuary:  0.87033\n",
      "epoch:  456  -  cost:  0.514189  - MSE:  36.4281301603  - Train Accuary:  0.87033\n",
      "epoch:  457  -  cost:  0.513389  - MSE:  36.4327809434  - Train Accuary:  0.872527\n",
      "epoch:  458  -  cost:  0.512591  - MSE:  36.4374371612  - Train Accuary:  0.872527\n",
      "epoch:  459  -  cost:  0.511796  - MSE:  36.4420676918  - Train Accuary:  0.872527\n",
      "epoch:  460  -  cost:  0.511003  - MSE:  36.4466528531  - Train Accuary:  0.872527\n",
      "epoch:  461  -  cost:  0.510214  - MSE:  36.4512754909  - Train Accuary:  0.872527\n",
      "epoch:  462  -  cost:  0.509426  - MSE:  36.4559014418  - Train Accuary:  0.872527\n",
      "epoch:  463  -  cost:  0.508642  - MSE:  36.4605196705  - Train Accuary:  0.872527\n",
      "epoch:  464  -  cost:  0.507859  - MSE:  36.4651300005  - Train Accuary:  0.872527\n",
      "epoch:  465  -  cost:  0.507078  - MSE:  36.4697706886  - Train Accuary:  0.872527\n",
      "epoch:  466  -  cost:  0.506299  - MSE:  36.4744768912  - Train Accuary:  0.872527\n",
      "epoch:  467  -  cost:  0.505523  - MSE:  36.4791540745  - Train Accuary:  0.872527\n",
      "epoch:  468  -  cost:  0.504749  - MSE:  36.4838496951  - Train Accuary:  0.872527\n",
      "epoch:  469  -  cost:  0.503979  - MSE:  36.4885290074  - Train Accuary:  0.87033\n",
      "epoch:  470  -  cost:  0.503212  - MSE:  36.4930173025  - Train Accuary:  0.87033\n",
      "epoch:  471  -  cost:  0.502448  - MSE:  36.497555017  - Train Accuary:  0.87033\n",
      "epoch:  472  -  cost:  0.501687  - MSE:  36.5020872658  - Train Accuary:  0.87033\n",
      "epoch:  473  -  cost:  0.500927  - MSE:  36.5066179721  - Train Accuary:  0.87033\n",
      "epoch:  474  -  cost:  0.500169  - MSE:  36.5110028263  - Train Accuary:  0.872527\n",
      "epoch:  475  -  cost:  0.499412  - MSE:  36.515454583  - Train Accuary:  0.872527\n",
      "epoch:  476  -  cost:  0.498658  - MSE:  36.5198949514  - Train Accuary:  0.874725\n",
      "epoch:  477  -  cost:  0.497906  - MSE:  36.5243377123  - Train Accuary:  0.874725\n",
      "epoch:  478  -  cost:  0.497156  - MSE:  36.5287535522  - Train Accuary:  0.874725\n",
      "epoch:  479  -  cost:  0.496409  - MSE:  36.5331957632  - Train Accuary:  0.874725\n",
      "epoch:  480  -  cost:  0.495662  - MSE:  36.5376146754  - Train Accuary:  0.874725\n",
      "epoch:  481  -  cost:  0.494915  - MSE:  36.5417418059  - Train Accuary:  0.874725\n",
      "epoch:  482  -  cost:  0.494169  - MSE:  36.5458745154  - Train Accuary:  0.874725\n",
      "epoch:  483  -  cost:  0.493426  - MSE:  36.5499887649  - Train Accuary:  0.874725\n",
      "epoch:  484  -  cost:  0.492687  - MSE:  36.5541651856  - Train Accuary:  0.874725\n",
      "epoch:  485  -  cost:  0.491951  - MSE:  36.5583347689  - Train Accuary:  0.874725\n",
      "epoch:  486  -  cost:  0.491216  - MSE:  36.56252379  - Train Accuary:  0.874725\n",
      "epoch:  487  -  cost:  0.49048  - MSE:  36.5667059363  - Train Accuary:  0.874725\n",
      "epoch:  488  -  cost:  0.489741  - MSE:  36.5709473114  - Train Accuary:  0.874725\n",
      "epoch:  489  -  cost:  0.489004  - MSE:  36.5751967528  - Train Accuary:  0.874725\n",
      "epoch:  490  -  cost:  0.488269  - MSE:  36.5794307575  - Train Accuary:  0.874725\n",
      "epoch:  491  -  cost:  0.487537  - MSE:  36.5836555167  - Train Accuary:  0.874725\n",
      "epoch:  492  -  cost:  0.486807  - MSE:  36.5879330324  - Train Accuary:  0.874725\n",
      "epoch:  493  -  cost:  0.486079  - MSE:  36.5922324417  - Train Accuary:  0.874725\n",
      "epoch:  494  -  cost:  0.485353  - MSE:  36.5965466056  - Train Accuary:  0.874725\n",
      "epoch:  495  -  cost:  0.484629  - MSE:  36.6008428247  - Train Accuary:  0.874725\n",
      "epoch:  496  -  cost:  0.483908  - MSE:  36.6051234238  - Train Accuary:  0.874725\n",
      "epoch:  497  -  cost:  0.483189  - MSE:  36.609426308  - Train Accuary:  0.874725\n",
      "epoch:  498  -  cost:  0.482472  - MSE:  36.6137101678  - Train Accuary:  0.874725\n",
      "epoch:  499  -  cost:  0.481758  - MSE:  36.6179987005  - Train Accuary:  0.874725\n",
      "Model saved in file: C:/Users/Nill/Desktop/conda_jupyter/data.cvv_models/TFM\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ed41f1f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ed3b0550b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.894737\n",
      "MSE: 36.6180\n"
     ]
    }
   ],
   "source": [
    "#initialize all the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#saver object to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#call the defined model\n",
    "y = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "#define the cost function (loss function)\n",
    "cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_out))\n",
    "#optimizer\n",
    "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "#create session object to launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#calculate the cost and accuracy for each epoch\n",
    "mse_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    #first run the trainin step by feeding values of X_train and Y_train\n",
    "    sess.run(training_step, feed_dict={x: X_train, y_out: Y_train})\n",
    "    #we calaculate the cost or loss function by feeding X_train and Y_train\n",
    "    cost = sess.run(cost_function, feed_dict={x: X_train, y_out: Y_train})\n",
    "    #it will append cost_history and cost\n",
    "    cost_history = np.append(cost_history, cost)\n",
    "    #we will find out correct prediction (the arguments are bascially the differencte between the actual and model output)\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_out, 1))\n",
    "    #calculate accuracy here\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    #get the predictions on the test data\n",
    "    pred_y = sess.run(y, feed_dict={x: X_test})\n",
    "    #mean square error gives the difference betewen predicted value and test value\n",
    "    mse = tf.reduce_mean(tf.square(pred_y - Y_test))\n",
    "    #launch the graph\n",
    "    mse_out = sess.run(mse)\n",
    "    #this will keep updating value for every epoch\n",
    "    mse_history.append(mse_out)\n",
    "    #get the accuracy on the training data\n",
    "    accuracy = (sess.run(accuracy, feed_dict={x: X_train, y_out: Y_train}))\n",
    "    #keep appending the accuracy\n",
    "    accuracy_history.append(accuracy)\n",
    "    \n",
    "    print('epoch: ', epoch,' - ', 'cost: ',cost,' - MSE: ', mse_out, ' - Train Accuary: ', accuracy)\n",
    "    \n",
    "#save the model in model path\n",
    "save_path = saver.save(sess, model_path)\n",
    "print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "#plot mse and accuaracy graph\n",
    "plt.plot(mse_history, 'r')\n",
    "plt.show()\n",
    "plt.plot(accuracy_history)\n",
    "plt.show()\n",
    "\n",
    "#print the final accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_out, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Test Accuracy: \", (sess.run(accuracy, feed_dict={x: X_test, y_out: Y_test})))\n",
    "\n",
    "#print final mean square error\n",
    "pred_y = sess.run(y, feed_dict={x: X_test})\n",
    "mse = tf.reduce_mean(tf.square(pred_y - Y_test))\n",
    "print(\"MSE: %.4f\" % sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Nill/Desktop/conda_jupyter/data.cvv_models/TFM\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "saver.restore(sess, model_path)\n",
    "\n",
    "prediction = tf.argmax(y, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(y_out, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************\n",
      " 0 Stands for B i.e. Benign & 1 Stands for M i.e. Malignant\n",
      "******************************************************\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 1.  0.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n",
      "Original Class :  [ 0.  1.]  Predicted Values :  0\n"
     ]
    }
   ],
   "source": [
    "#making predictions\n",
    "\n",
    "print('******************************************************')\n",
    "print(\" 0 Stands for B i.e. Benign & 1 Stands for M i.e. Malignant\")\n",
    "print('******************************************************')\n",
    "for i in range(500, 520):\n",
    "    prediction_run = sess.run(prediction, feed_dict={x: X[i].reshape(1, 30)})\n",
    "    accuracy_run = sess.run(accuracy, feed_dict={x: X[i].reshape(1, 30), y_out: Y[i].reshape(1, 2)})\n",
    "    print(\"Original Class : \", Y[i], \" Predicted Values : \", prediction_run[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
